{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import sklearn.preprocessing as skp\n",
    "# import np_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Code\n",
    "This part can be ignored, it serves to build the technical process to develop the actual model, but doesn't contain too much to interpret from a model evaluation point of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates Lagged series\n",
    "#Goes through a series and generates an lag+1 dimensional pandas DataFrame that has each previous lag timeunit\n",
    "#as a column and current as the last cobilumn\n",
    "#Input: Pandas Series\n",
    "#Output: lag+1 dimensional DataFrame\n",
    "\n",
    "def timeseriesLagged(data, lag=60):\n",
    "    df = data\n",
    "    columns = [df.shift(i) for i in range(1, lag+2)] \n",
    "    df = pd.concat(columns,axis=1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.columns = [str(lag+2-x) for x in range(1,lag+2)]\n",
    "    # df.reset_index(inplace=True,drop=False)\n",
    "    df = df[df.columns[::-1]] #Flip because we want newer data on the right\n",
    "    df= df.iloc[lag+1:] # drop the first 'lag' columns because zeroes.\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    return df\n",
    "\n",
    "# Binarizes the last column into 1, 0, -1. 1 = buy 0 = do nothing -1 = sell\n",
    "# Rate is the percent increase or decrease that should trigger a buy or a sell\n",
    "# lag is the time unit of lag. \n",
    "# atleast is how many of the lookahead need to be atleast the same or greater than flat+rat\n",
    "# Input: lagged pandas DataFrame, uint lag, double dif, double flat, double atleast between 0 and 1\n",
    "# Output : Pandas Dataframe with last column binarized\n",
    "def binarizeTime(resLagged,rate = 0,lookahead = 0, flat = 0,atleast = 0.5):\n",
    "    if lookahead <= 0 :\n",
    "        raise Exception(\"lookahead Must be 1 or higher!\")\n",
    "    resLagged = resLagged.copy() # Make a deep copy\n",
    "    last = np.shape(resLagged)[1] # find the length of the data \n",
    "    last = last-lookahead # convert it to string for loc\n",
    "    colsLookAhead = list(resLagged.loc[:,str(last+1):str(last + lookahead)])\n",
    "    colsLast = resLagged[str(last)]\n",
    "    diffs = resLagged[colsLookAhead].subtract(colsLast,axis=0)\n",
    "#     print(diffs)\n",
    "    greater = diffs>=flat  # all the times the price changed higer than flat\n",
    "    greater = np.count_nonzero(greater,axis=1).reshape((1,-1))\n",
    "    lesser = diffs<=-flat # all the times the price fell lower than fat\n",
    "    lesser = np.count_nonzero(lesser,axis=1).reshape((1,-1))\n",
    "#     return greater,lesser\n",
    "#     print(greater)\n",
    "    greater = greater.reshape(1,-1)\n",
    "    changeToBuy = np.any(greater > lesser & np.greater(greater,atleast*lookahead),axis=0) # make sure more rises than falls and atleast half rises\n",
    "    changeToSell = np.any(lesser > greater & np.greater(lesser,atleast*lookahead),axis=0)      # make sure more falls than rises and atleast half rises\n",
    "    changeToHold = ~changeToBuy & ~changeToSell\n",
    "    resLagged = resLagged.drop(colsLookAhead,1)\n",
    "    resLagged.loc[changeToSell,str(last+1)] = -1 # Set sell to -1\n",
    "    resLagged.loc[changeToBuy,str(last+1)] = 1 # Set buy to 1\n",
    "    resLagged.loc[changeToHold,str(last+1)] = 0 # Set to 0\n",
    "    return resLagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nifty. Reading and Cleaning\n",
    "h = 5\n",
    "fut = pd.read_csv(\"Nifty50FUT.csv\")\n",
    "fut['Price']= fut['Price'].str.replace(\",\",\"\").astype(np.double).rolling(h).mean()\n",
    "fut['Open']= fut['Open'].str.replace(\",\",\"\").astype(np.double).rolling(h).mean()\n",
    "fut['High']= fut['High'].str.replace(\",\",\"\").astype(np.double).rolling(h).mean()\n",
    "fut['Low']= fut['Low'].str.replace(\",\",\"\").astype(np.double).rolling(h).mean()\n",
    "fut['Vol.']= fut['Vol.'].str.replace(\",\",\"\").str.replace(\"M\",\"e6\").str.replace(\"-\",\"0\").str.replace(\"K\",\"e3\").astype(np.double).rolling(h).mean()\n",
    "fut = fut[::-1]\n",
    "fut.reset_index(inplace=True, drop = True)\n",
    "\n",
    "nifty = pd.read_csv(\"Nifty.csv\")\n",
    "nifty = nifty[::-1]\n",
    "nifty.reset_index(inplace=True, drop = True)\n",
    "nifty['Price']= nifty['Price'].str.replace(\",\",\"\").astype(np.double).rolling(h).mean()\n",
    "nifty['Open']= nifty['Open'].str.replace(\",\",\"\").astype(np.double).rolling(h).mean()\n",
    "nifty['High']= nifty['High'].str.replace(\",\",\"\").astype(np.double).rolling(h).mean()\n",
    "nifty['Low']= nifty['Low'].str.replace(\",\",\"\").astype(np.double).rolling(h).mean()\n",
    "nifty['Vol.']= nifty['Vol.'].str.replace(\",\",\"\").str.replace(\"M\",\"e6\").str.replace(\"-\",\"0\").str.replace(\"K\",\"e3\").astype(np.double).rolling(h).mean()\n",
    "\n",
    "\n",
    "niftyDrop = np.setdiff1d(fut['Date'].values,nifty['Date'].values)\n",
    "futDrop = np.setdiff1d(nifty['Date'].values,fut['Date'].values)\n",
    "nifty = nifty[~nifty.Date.isin(futDrop)]\n",
    "fut = fut[~fut.Date.isin(niftyDrop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              NaN\n",
       "1              NaN\n",
       "2              NaN\n",
       "3        6237.6750\n",
       "4        6250.8500\n",
       "5        6272.4000\n",
       "6        6246.5125\n",
       "7        6225.3500\n",
       "8        6213.3375\n",
       "9        6165.0875\n",
       "10       6111.8500\n",
       "11       6043.8875\n",
       "12       5920.2875\n",
       "13       5701.9000\n",
       "14       5438.6375\n",
       "15       5247.4250\n",
       "16       5066.1500\n",
       "17       5114.1625\n",
       "18       5208.0250\n",
       "19       5235.8375\n",
       "20       5273.7125\n",
       "21       5210.6000\n",
       "22       5223.2875\n",
       "23       5267.4750\n",
       "24       5348.5250\n",
       "25       5388.3500\n",
       "26       5333.1500\n",
       "27       5241.9625\n",
       "28       5074.3625\n",
       "29       4949.0125\n",
       "           ...    \n",
       "2713    11019.1000\n",
       "2714    11059.9500\n",
       "2715    11100.6625\n",
       "2716    11136.2750\n",
       "2717    11188.1000\n",
       "2718    11237.2250\n",
       "2719    11296.0250\n",
       "2720    11347.7750\n",
       "2721    11341.6750\n",
       "2722    11355.9875\n",
       "2723    11366.6500\n",
       "2724    11377.6625\n",
       "2725    11424.0000\n",
       "2726    11446.9500\n",
       "2727    11453.2000\n",
       "2728    11443.9000\n",
       "2729    11442.8500\n",
       "2731    11423.6875\n",
       "2732    11435.9875\n",
       "2733    11485.4375\n",
       "2734    11516.2125\n",
       "2736    11562.3375\n",
       "2737    11583.1250\n",
       "2738    11614.7250\n",
       "2739    11656.2000\n",
       "2740    11684.1250\n",
       "2741    11710.4250\n",
       "2742    11717.9125\n",
       "2743    11690.2375\n",
       "2744    11670.0875\n",
       "Name: Price, Length: 2642, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fut['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nifty Base\n",
    "volNifty = nifty['Vol.'].diff().dropna()\n",
    "openNifty = nifty['Open'].diff().dropna()\n",
    "highNifty = nifty['High'].diff().dropna()\n",
    "lowNifty = nifty['Low'].diff().dropna()\n",
    "dataNifty = nifty['Price'].diff().dropna()\n",
    "\n",
    "# Nifty Futs\n",
    "volFut = fut['Vol.'].diff().dropna()\n",
    "openFut = fut['Open'].diff().dropna()\n",
    "highFut = fut['High'].diff().dropna()\n",
    "lowFut = fut['Low'].diff().dropna()\n",
    "dataFut = fut['Price'].diff().dropna()\n",
    "#Future Premium\n",
    "prems = dataFut.values - dataNifty.values\n",
    "prems = pd.Series(prems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag=15\n",
    "lookahead = 1\n",
    "flat = 0\n",
    "# First N predicts N+1th. Creating the 1st N series\n",
    "closeNifty = timeseriesLagged(dataNifty,lag + lookahead-1)\n",
    "# These are correlated with closeNifty, so we will ignore them for now\n",
    "# openNifty = timeseriesLagged(openNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# highNifty = timeseriesLagged(highNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# lowNifty = timeseriesLagged(lowNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "\n",
    "volNifty = timeseriesLagged(volNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "volNifty = skp.minmax_scale(volNifty,axis=1)\n",
    "\n",
    "# First N predicts N+1th. Creating the 1st N series\n",
    "closeFut = timeseriesLagged(dataFut,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "closeFut = skp.minmax_scale(closeFut,axis=1)\n",
    "\n",
    "# These are correlated with closeNifty, so we will ignore them for now\n",
    "# openNifty = timeseriesLagged(openNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# highNifty = timeseriesLagged(highNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# lowNifty = timeseriesLagged(lowNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "volFut = timeseriesLagged(volFut,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "volFut = skp.minmax_scale(volFut,axis=1)\n",
    "\n",
    "prems = timeseriesLagged(prems,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "prems = skp.minmax_scale(prems,axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeNifty.loc[closeNifty[str(lag+1)] > flat,str(lag+1)] = 1\n",
    "closeNifty.loc[closeNifty[str(lag+1)] <= flat,str(lag+1)] = 2\n",
    "closeNifty.loc[closeNifty[str(lag+1)] == flat,str(lag+1)] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "buySeriesLabs = closeNifty[str(lag+1)] # labels\n",
    "buySeriesFeats = closeNifty.drop(str(lag+1),axis=1) #features\n",
    "buySeriesFeats = buySeriesFeats.values\n",
    "buySeriesFeats = skp.scale(buySeriesFeats,axis=1)\n",
    "\n",
    "buySeries = np.zeros((len(buySeriesFeats),buySeriesFeats.shape[-1],n))\n",
    "buySeries[:,:,0] = buySeriesFeats\n",
    "buySeries[:,:,1] = volFut\n",
    "buySeries[:,:,2] = prems\n",
    "buySeries[:,:,3] = closeFut\n",
    "buySeries[:,:,4] = volNifty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = buySeries,buySeriesLabs\n",
    "x,y = shuffle(x,y)\n",
    "tot = len(x)\n",
    "y = y.values\n",
    "yOrig = np.copy(y)\n",
    "\n",
    "x = x.reshape(len(x),-1)\n",
    "trainPercent = 0.9 # majority of data used for training\n",
    "testPercent = 0.9 # \n",
    "valPercent = 1.00  #\n",
    "\n",
    "# Test Train Val Split\n",
    "\n",
    "xTrain = x[0:int(trainPercent*tot),:]\n",
    "yTrain = y[0:int(trainPercent*tot)]\n",
    "\n",
    "xTest = x[int(trainPercent*tot): int(testPercent*tot),:]\n",
    "yTest = y[int(trainPercent*tot): int(testPercent*tot)]\n",
    "\n",
    "xVal = x[int(testPercent*tot):,:]\n",
    "yVal = y[int(testPercent*tot):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(xTrain, yTrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4866920152091255\n",
      "[[89 44]\n",
      " [91 39]]\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(xVal)\n",
    "print(sum(preds==yVal)/len(yVal))\n",
    "print(confusion_matrix(yVal,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Dataset\n",
    "We will try to use Open Interest, Volume, Price from Nifty Futures and Price and Volume from Nifty from the last $LAG$ (variable, will adjust as fit) to make a prediction on whether nifty will increase or decrease as compared to $h$ days ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the data from the csv files\n",
    "nifty = pd.read_csv(\"NiftyPrice.csv\")\n",
    "nifty = nifty[::-1] # Reverse to align\n",
    "nifty.reset_index(inplace=True, drop = True)\n",
    "nifty.rename(columns={ nifty.columns[0]: \"Date\" },inplace=True)\n",
    "\n",
    "futOI = pd.read_csv(\"NiftyFutOI.csv\")\n",
    "futOI = futOI[::-1] # Reverse to align\n",
    "futOI.reset_index(inplace=True, drop = True)\n",
    "futOI.rename(columns={ futOI.columns[0]: \"Date\" },inplace=True)\n",
    "\n",
    "fut = pd.read_csv(\"NiftyFutPrice.csv\")\n",
    "fut = fut[::-1] # reverse to align\n",
    "fut.reset_index(inplace=True, drop = True)\n",
    "fut.rename(columns={ fut.columns[0]: \"Date\" },inplace=True)\n",
    "\n",
    "oil = pd.read_csv(\"oil.csv\")\n",
    "# oil = oil[::-1]\n",
    "oil.reset_index(inplace=True, drop = True)\n",
    "oil.rename(columns={ oil.columns[0]: \"Date\" },inplace=True)\n",
    "\n",
    "usdinr = pd.read_csv(\"usdinr.csv\")\n",
    "usdinr.rename(columns={ usdinr.columns[0]: \"Date\" },inplace=True)\n",
    "\n",
    "advances = pd.read_csv(\"advances.csv\")\n",
    "advances.rename(columns={ advances.columns[0]: \"Date\" },inplace=True)\n",
    "\n",
    "declines = pd.read_csv(\"decline.csv\")\n",
    "declines.rename(columns={ declines.columns[0]: \"Date\" },inplace=True)\n",
    "# Drop any rows not common in both\n",
    "# Data Arrays\n",
    "dat = [nifty,fut,futOI,oil,usdinr,advances,declines]\n",
    "\n",
    "# Clean and make sure they have the same dates\n",
    "dropset = [] # Collect the dropsets\n",
    "n = 2\n",
    "for cleanee in dat: # O(n^2)\n",
    "    for cleaner in dat: \n",
    "        drop = np.setdiff1d(cleaner['Date'].values,cleanee['Date'].values) # Everything in cleaner AND not in cleanee\n",
    "        dropset.append(drop)\n",
    "\n",
    "for i in range(0,len(dat)): # Actuall drop\n",
    "    for drop in dropset:\n",
    "        dat[i] = dat[i][~dat[i].Date.isin(drop)]        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowestVolNiftyIndex = min(volNiftyt4.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create % change daily bars of the different features\n",
    "# Nifty Base\n",
    "h = 1 # change from how far ago\n",
    "t = 5\n",
    "t2 = 9\n",
    "t4 = 15 \n",
    "volNifty = dat[0]['PX_VOLUME'].pct_change(h).dropna().rolling(t).mean().dropna()\n",
    "volNiftyt2 = dat[0]['PX_VOLUME'].pct_change(h).dropna().rolling(t2).mean().dropna()\n",
    "volNiftyt4 = dat[0]['PX_VOLUME'].pct_change(h).dropna().rolling(t4).mean().dropna()\n",
    "lowestVolNiftyIndex = min(volNiftyt4.index)\n",
    "\n",
    "dataNifty = dat[0]['PX_LAST'].pct_change(h).dropna().rolling(t).mean().dropna()\n",
    "dataNiftyt2 = dat[0]['PX_LAST'].pct_change(h).dropna().rolling(t2).mean().dropna()\n",
    "dataNiftyt4 = dat[0]['PX_LAST'].pct_change(h).dropna().rolling(t4).mean().dropna()\n",
    "\n",
    "# Nifty Futs\n",
    "volFut = dat[1]['PX_VOLUME'].pct_change(h).dropna().rolling(t).mean().dropna()\n",
    "volFutt2 = dat[1]['PX_VOLUME'].pct_change(h).dropna().rolling(t2).mean().dropna()\n",
    "volFutt4 = dat[1]['PX_VOLUME'].pct_change(h).dropna().rolling(t4).mean().dropna()\n",
    "\n",
    "oiFut = dat[2]['OPEN_INT'].pct_change(h).dropna().rolling(t).mean().dropna()\n",
    "oiFutt2 = dat[2]['OPEN_INT'].pct_change(h).dropna().rolling(t2).mean().dropna()\n",
    "oiFutt4 = dat[2]['OPEN_INT'].pct_change(h).dropna().rolling(t4).mean().dropna()\n",
    "\n",
    "dataFut = dat[1]['PX_LAST'].pct_change(h).dropna().rolling(t).mean().dropna()\n",
    "dataFutt2 = dat[1]['PX_LAST'].pct_change(h).dropna().rolling(t2).mean().dropna()\n",
    "dataFutt4 = dat[1]['PX_LAST'].pct_change(h).dropna().rolling(t4).mean().dropna()\n",
    "\n",
    "# Oil\n",
    "volOil = dat[3]['PX_VOLUME'].pct_change(h).dropna().rolling(t).mean().dropna()\n",
    "volOilt2 = dat[3]['PX_VOLUME'].pct_change(h).dropna().rolling(t2).mean().dropna()\n",
    "volOilt4 = dat[3]['PX_VOLUME'].pct_change(h).dropna().rolling(t4).mean().dropna()\n",
    "\n",
    "dataOil = dat[3]['PX_LAST'].pct_change(h).dropna().rolling(t).mean().dropna()\n",
    "dataOilt2 = dat[3]['PX_LAST'].pct_change(h).dropna().rolling(t4).mean().dropna()\n",
    "dataOilt4 = dat[3]['PX_LAST'].pct_change(h).dropna().rolling(t4).mean().dropna()\n",
    "\n",
    "\n",
    "# USDINR\n",
    "dataUSDINR = dat[4]['Last Price'].astype(np.float).pct_change(h).dropna().rolling(t).mean().dropna()\n",
    "dataUSDINRt2 = dat[4]['Last Price'].astype(np.float).pct_change(h).dropna().rolling(t2).mean().dropna()\n",
    "dataUSDINRt4 = dat[4]['Last Price'].astype(np.float).pct_change(h).dropna().rolling(t4).mean().dropna()\n",
    "\n",
    "# Advance Decline\n",
    "advanceTot = dat[5]['PX_LAST'].rolling(h).mean()\n",
    "declineTot = dat[6]['PX_LAST'].rolling(h).mean()\n",
    "# advanceDecline = advanceDecline # drop the first row because we are dropping it in the others\n",
    "advanceDecline = advanceTot/declineTot\n",
    "advanceDecline = pd.Series(advanceDecline[t:])\n",
    "# advanceDecline = advanceDecline.pct_change(h).dropna()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 15\n",
    "lookahead = 1\n",
    "flat = 0\n",
    "# First N predicts N+1th. Creating the 1st N series\n",
    "closeNifty = timeseriesLagged(dataNifty,lag + lookahead-1)\n",
    "\n",
    "volNifty = timeseriesLagged(volNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# volNifty = skp.scale(volNifty,axis=1)\n",
    "\n",
    "# First N predicts N+1th. Creating the 1st N series\n",
    "closeFut = timeseriesLagged(dataFut,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# closeFut = skp.scale(closeFut,axis=1)\n",
    "\n",
    "volFut = timeseriesLagged(volFut,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# volFut = skp.scale(volFut,axis=1)\n",
    "\n",
    "oiFut = timeseriesLagged(oiFut,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# oiFut = skp.scale(oiFut,axis=1)\n",
    "\n",
    "# Oil\n",
    "volOil = timeseriesLagged(volOil,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# volOil = skp.scale(volOil,axis=1)\n",
    "dataOil = timeseriesLagged(dataOil,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# dataOil = skp.scale(dataOil,axis=1)\n",
    "\n",
    "# USDINR\n",
    "dataUSDINR = timeseriesLagged(dataUSDINR,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# dataUSDINR = skp.scale(dataUSDINR,axis=1)\n",
    "\n",
    "\n",
    "# Advance Decline\n",
    "advanceDecline = timeseriesLagged(advanceDecline,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "advanceDecline = skp.scale(advanceDecline,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binazrize the last column\n",
    "toOne = closeNifty[str(lag+1)] > closeNifty[str(lag)]\n",
    "toTwo = closeNifty[str(lag+1)] <= closeNifty[str(lag)]\n",
    "closeNifty.loc[toOne,str(lag+1)] = 1\n",
    "closeNifty.loc[toTwo,str(lag+1)] = 2\n",
    "# closeNifty.loc[closeNifty[str(lag+1)] == closeNifty[str(lag)],str(lag+1)] = 3\n",
    "closeNifty[str(lag+1)]=closeNifty[str(lag+1)].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the feature vector\n",
    "n = 4\n",
    "buySeriesLabs = closeNifty[str(lag+1)] # labels\n",
    "buySeriesFeats = closeNifty.drop(str(lag+1),axis=1) #features\n",
    "buySeriesFeats = buySeriesFeats.values\n",
    "# buySeriesFeats = skp.scale(buySeriesFeats,axis=1)\n",
    "\n",
    "\n",
    "buySeries = np.zeros((len(buySeriesFeats),buySeriesFeats.shape[-1],n))\n",
    "buySeries[:,:,0] = buySeriesFeats\n",
    "buySeries[:,:,1] = volFut\n",
    "buySeries[:,:,2] = oiFut\n",
    "# buySeries[:,:,3] = volOil\n",
    "# buySeries[:,:,4] = dataOil\n",
    "# buySeries[:,:,5] = dataUSDINR\n",
    "buySeries[:,:,3] = advanceDecline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train test val\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x,y = buySeries,buySeriesLabs\n",
    "x,y = shuffle(x,y)\n",
    "tot = len(x)\n",
    "y = y.values\n",
    "yOrig = np.copy(y)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "\n",
    "\n",
    "x = x.reshape(len(x),-1)\n",
    "trainPercent = 0.7 # majority of data used for training\n",
    "testPercent = 0.7 # \n",
    "valPercent = 1.00  #\n",
    "\n",
    "# Test Train Val Split\n",
    "\n",
    "xTrain = x[0:int(trainPercent*tot),:]\n",
    "yTrain = y[0:int(trainPercent*tot)]\n",
    "scaler.fit(xTrain)\n",
    "\n",
    "xTest = x[int(trainPercent*tot): int(testPercent*tot),:]\n",
    "yTest = y[int(trainPercent*tot): int(testPercent*tot)]\n",
    "\n",
    "xVal = x[int(testPercent*tot):,:]\n",
    "yVal = y[int(testPercent*tot):]\n",
    "\n",
    "# xTrain = scaler.transform(xTrain)  \n",
    "# xTest = scaler.transform(xVal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "neigh = KNeighborsClassifier(n_neighbors=15, weights= 'distance')\n",
    "neigh.fit(xTrain,yTrain)\n",
    "preds = neigh.predict(xVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predsArray = []\n",
    "# accArray = []\n",
    "# test = 500\n",
    "# for i in range(100,test):\n",
    "#     neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "#     neigh.fit(xTrain,yTrain)\n",
    "#     pred = neigh.predict(xVal)\n",
    "#     acc = sum(yVal==pred)/len(yVal)\n",
    "#     predsArray.append(pred)\n",
    "#     accArray.append(acc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE NEGATIVE: 203\n",
      "TRUE POSITIVE: 193\n",
      "FALSE POSITIVE: 101\n",
      "FALSE NEGATIVE: 130\n",
      "Weight of Buy in data set: 0.484848\n"
     ]
    }
   ],
   "source": [
    "conf = confusion_matrix(yVal,preds)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "print(\"TRUE NEGATIVE: %d\" % tn)\n",
    "print(\"TRUE POSITIVE: %d\" % tp)\n",
    "print(\"FALSE POSITIVE: %d\" % fp)\n",
    "print(\"FALSE NEGATIVE: %d\" % fn)\n",
    "print(\"Weight of Buy in data set: %f\" % (sum(yVal==1)/len(yVal)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Percent: 0.631579\n",
      "Precision of Buy Model= 0.656463\n",
      "Recall of Buy Model = 0.597523\n"
     ]
    }
   ],
   "source": [
    "# Precision Recall of Sell\n",
    "print(\"Accuracy Percent: %f\"% (sum(preds==yVal)/len(yVal)))\n",
    "print(\"Precision of Buy Model= %f\" % (tp/(tp+fp)))\n",
    "print(\"Recall of Buy Model = %f\" % (tp/(tp+fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Percent: 0.631579\n",
      "Precision of Sell Model= 0.609610\n",
      "Recall of Sell Model = 0.667763\n",
      "Weight of Sell in data set: 0.515152\n"
     ]
    }
   ],
   "source": [
    "# Precision Recall of Sell\n",
    "print(\"Accuracy Percent: %f\"% (sum(preds==yVal)/len(yVal)))\n",
    "print(\"Precision of Sell Model= %f\" % (tn/(tn+fn)))\n",
    "print(\"Recall of Sell Model = %f\" % (tn/(tn+fp)))\n",
    "print(\"Weight of Sell in data set: %f\" % (sum(yVal==2)/len(yVal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rbf SVM\n",
    "clf = svm.SVC(kernel='linear', degree = 3, probability= True) # change kernels between linear, poly and rbf as fit.\n",
    "clf.fit(xTrain, yTrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predsProbs = clf.predict_log_proba(xVal)\n",
    "# np.argmax(predsProbs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsProbs = clf.predict_proba(xVal)\n",
    "preds = clf.predict(xVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predsProbs\n",
    "# predsProbs = predsProbs < 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = np.argmax(predsProbs,axis=1) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE NEGATIVE: 290\n",
      "TRUE POSITIVE: 101\n",
      "FALSE POSITIVE: 14\n",
      "FALSE NEGATIVE: 222\n",
      "Weight of Buy in data set: 0.484848\n"
     ]
    }
   ],
   "source": [
    "conf = confusion_matrix(yVal,preds)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "print(\"TRUE NEGATIVE: %d\" % tn)\n",
    "print(\"TRUE POSITIVE: %d\" % tp)\n",
    "print(\"FALSE POSITIVE: %d\" % fp)\n",
    "print(\"FALSE NEGATIVE: %d\" % fn)\n",
    "print(\"Weight of Buy in data set: %f\" % (sum(yVal==1)/len(yVal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Percent: 0.623604\n",
      "Precision of Buy Model= 0.878261\n",
      "Recall of Buy Model = 0.312693\n"
     ]
    }
   ],
   "source": [
    "# Precision Recall of Sell\n",
    "print(\"Accuracy Percent: %f\"% (sum(preds==yVal)/len(yVal)))\n",
    "print(\"Precision of Buy Model= %f\" % (tp/(tp+fp)))\n",
    "print(\"Recall of Buy Model = %f\" % (tp/(tp+fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Percent: 0.623604\n",
      "Precision of Sell Model= 0.566406\n",
      "Recall of Sell Model = 0.953947\n",
      "Weight of Sell in data set: 0.515152\n"
     ]
    }
   ],
   "source": [
    "# Precision Recall of Sell\n",
    "print(\"Accuracy Percent: %f\"% (sum(preds==yVal)/len(yVal)))\n",
    "print(\"Precision of Sell Model= %f\" % (tn/(tn+fn)))\n",
    "print(\"Recall of Sell Model = %f\" % (tn/(tn+fp)))\n",
    "print(\"Weight of Sell in data set: %f\" % (sum(yVal==2)/len(yVal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
