{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import sklearn.preprocessing as skp\n",
    "# import np_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Code\n",
    "This part can be ignored, it serves to build the technical process to develop the actual model, but doesn't contain too much to interpret from a model evaluation point of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates Lagged series\n",
    "#Goes through a series and generates an lag+1 dimensional pandas DataFrame that has each previous lag timeunit\n",
    "#as a column and current as the last cobilumn\n",
    "#Input: Pandas Series\n",
    "#Output: lag+1 dimensional DataFrame\n",
    "\n",
    "def timeseriesLagged(data, lag=60):\n",
    "    df = data\n",
    "    columns = [df.shift(i) for i in range(1, lag+2)] \n",
    "    df = pd.concat(columns,axis=1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.columns = [str(lag+2-x) for x in range(1,lag+2)]\n",
    "    # df.reset_index(inplace=True,drop=False)\n",
    "    df = df[df.columns[::-1]] #Flip because we want newer data on the right\n",
    "    df= df.iloc[lag+1:] # drop the first 'lag' columns because zeroes.\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    return df\n",
    "\n",
    "# Binarizes the last column into 1, 0, -1. 1 = buy 0 = do nothing -1 = sell\n",
    "# Rate is the percent increase or decrease that should trigger a buy or a sell\n",
    "# lag is the time unit of lag. \n",
    "# atleast is how many of the lookahead need to be atleast the same or greater than flat+rat\n",
    "# Input: lagged pandas DataFrame, uint lag, double dif, double flat, double atleast between 0 and 1\n",
    "# Output : Pandas Dataframe with last column binarized\n",
    "def binarizeTime(resLagged,rate = 0,lookahead = 0, flat = 0,atleast = 0.5):\n",
    "    if lookahead <= 0 :\n",
    "        raise Exception(\"lookahead Must be 1 or higher!\")\n",
    "    resLagged = resLagged.copy() # Make a deep copy\n",
    "    last = np.shape(resLagged)[1] # find the length of the data \n",
    "    last = last-lookahead # convert it to string for loc\n",
    "    colsLookAhead = list(resLagged.loc[:,str(last+1):str(last + lookahead)])\n",
    "    colsLast = resLagged[str(last)]\n",
    "    diffs = resLagged[colsLookAhead].subtract(colsLast,axis=0)\n",
    "#     print(diffs)\n",
    "    greater = diffs>=flat  # all the times the price changed higer than flat\n",
    "    greater = np.count_nonzero(greater,axis=1).reshape((1,-1))\n",
    "    lesser = diffs<=-flat # all the times the price fell lower than fat\n",
    "    lesser = np.count_nonzero(lesser,axis=1).reshape((1,-1))\n",
    "#     return greater,lesser\n",
    "#     print(greater)\n",
    "    greater = greater.reshape(1,-1)\n",
    "    changeToBuy = np.any(greater > lesser & np.greater(greater,atleast*lookahead),axis=0) # make sure more rises than falls and atleast half rises\n",
    "    changeToSell = np.any(lesser > greater & np.greater(lesser,atleast*lookahead),axis=0)      # make sure more falls than rises and atleast half rises\n",
    "    changeToHold = ~changeToBuy & ~changeToSell\n",
    "    resLagged = resLagged.drop(colsLookAhead,1)\n",
    "    resLagged.loc[changeToSell,str(last+1)] = -1 # Set sell to -1\n",
    "    resLagged.loc[changeToBuy,str(last+1)] = 1 # Set buy to 1\n",
    "    resLagged.loc[changeToHold,str(last+1)] = 0 # Set to 0\n",
    "    return resLagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nifty. Reading and Cleaning\n",
    "fut = pd.read_csv(\"Nifty50FUT.csv\")\n",
    "fut['Price']= fut['Price'].str.replace(\",\",\"\").astype(np.double)\n",
    "fut['Open']= fut['Open'].str.replace(\",\",\"\").astype(np.double)\n",
    "fut['High']= fut['High'].str.replace(\",\",\"\").astype(np.double)\n",
    "fut['Low']= fut['Low'].str.replace(\",\",\"\").astype(np.double)\n",
    "fut['Vol.']= fut['Vol.'].str.replace(\",\",\"\").str.replace(\"M\",\"e6\").str.replace(\"-\",\"0\").str.replace(\"K\",\"e3\").astype(np.double)\n",
    "fut = fut[::-1]\n",
    "fut.reset_index(inplace=True, drop = True)\n",
    "\n",
    "nifty = pd.read_csv(\"Nifty.csv\")\n",
    "nifty = nifty[::-1]\n",
    "nifty.reset_index(inplace=True, drop = True)\n",
    "nifty['Price']= nifty['Price'].str.replace(\",\",\"\").astype(np.double)\n",
    "nifty['Open']= nifty['Open'].str.replace(\",\",\"\").astype(np.double)\n",
    "nifty['High']= nifty['High'].str.replace(\",\",\"\").astype(np.double)\n",
    "nifty['Low']= nifty['Low'].str.replace(\",\",\"\").astype(np.double)\n",
    "nifty['Vol.']= nifty['Vol.'].str.replace(\",\",\"\").str.replace(\"M\",\"e6\").str.replace(\"-\",\"0\").str.replace(\"K\",\"e3\").astype(np.double)\n",
    "\n",
    "\n",
    "niftyDrop = np.setdiff1d(fut['Date'].values,nifty['Date'].values)\n",
    "futDrop = np.setdiff1d(nifty['Date'].values,fut['Date'].values)\n",
    "nifty = nifty[~nifty.Date.isin(futDrop)]\n",
    "fut = fut[~fut.Date.isin(niftyDrop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nifty Base\n",
    "volNifty = nifty['Vol.'].diff().dropna()\n",
    "openNifty = nifty['Open'].diff().dropna()\n",
    "highNifty = nifty['High'].diff().dropna()\n",
    "lowNifty = nifty['Low'].diff().dropna()\n",
    "dataNifty = nifty['Price'].diff().dropna()\n",
    "\n",
    "# Nifty Futs\n",
    "volFut = fut['Vol.'].diff().dropna()\n",
    "openFut = fut['Open'].diff().dropna()\n",
    "highFut = fut['High'].diff().dropna()\n",
    "lowFut = fut['Low'].diff().dropna()\n",
    "dataFut = fut['Price'].diff().dropna()\n",
    "#Future Premium\n",
    "prems = dataFut.values - dataNifty.values\n",
    "prems = pd.Series(prems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag=5\n",
    "lookahead = 1\n",
    "flat = 0\n",
    "# First N predicts N+1th. Creating the 1st N series\n",
    "closeNifty = timeseriesLagged(dataNifty,lag + lookahead-1)\n",
    "# These are correlated with closeNifty, so we will ignore them for now\n",
    "# openNifty = timeseriesLagged(openNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# highNifty = timeseriesLagged(highNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# lowNifty = timeseriesLagged(lowNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "\n",
    "volNifty = timeseriesLagged(volNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "volNifty = skp.minmax_scale(volNifty,axis=1)\n",
    "\n",
    "# First N predicts N+1th. Creating the 1st N series\n",
    "closeFut = timeseriesLagged(dataFut,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "closeFut = skp.minmax_scale(closeFut,axis=1)\n",
    "\n",
    "# These are correlated with closeNifty, so we will ignore them for now\n",
    "# openNifty = timeseriesLagged(openNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# highNifty = timeseriesLagged(highNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "# lowNifty = timeseriesLagged(lowNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "volFut = timeseriesLagged(volFut,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "volFut = skp.minmax_scale(volFut,axis=1)\n",
    "\n",
    "prems = timeseriesLagged(prems,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "prems = skp.minmax_scale(prems,axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeNifty.loc[closeNifty[str(lag+1)] > flat,str(lag+1)] = 1\n",
    "closeNifty.loc[closeNifty[str(lag+1)] <= flat,str(lag+1)] = 2\n",
    "closeNifty.loc[closeNifty[str(lag+1)] == flat,str(lag+1)] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "buySeriesLabs = closeNifty[str(lag+1)] # labels\n",
    "buySeriesFeats = closeNifty.drop(str(lag+1),axis=1) #features\n",
    "buySeriesFeats = buySeriesFeats.values\n",
    "buySeriesFeats = skp.scale(buySeriesFeats,axis=1)\n",
    "\n",
    "buySeries = np.zeros((len(buySeriesFeats),buySeriesFeats.shape[-1],n))\n",
    "buySeries[:,:,0] = buySeriesFeats\n",
    "buySeries[:,:,1] = volFut\n",
    "buySeries[:,:,2] = prems\n",
    "buySeries[:,:,3] = closeFut\n",
    "buySeries[:,:,4] = volNifty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = buySeries,buySeriesLabs\n",
    "x,y = shuffle(x,y)\n",
    "tot = len(x)\n",
    "y = y.values\n",
    "yOrig = np.copy(y)\n",
    "\n",
    "x = x.reshape(len(x),-1)\n",
    "trainPercent = 0.9 # majority of data used for training\n",
    "testPercent = 0.9 # \n",
    "valPercent = 1.00  #\n",
    "\n",
    "# Test Train Val Split\n",
    "\n",
    "xTrain = x[0:int(trainPercent*tot),:]\n",
    "yTrain = y[0:int(trainPercent*tot)]\n",
    "\n",
    "xTest = x[int(trainPercent*tot): int(testPercent*tot),:]\n",
    "yTest = y[int(trainPercent*tot): int(testPercent*tot)]\n",
    "\n",
    "xVal = x[int(testPercent*tot):,:]\n",
    "yVal = y[int(testPercent*tot):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(xTrain, yTrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5568181818181818\n",
      "[[106  46]\n",
      " [ 71  41]]\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(xVal)\n",
    "print(sum(preds==yVal)/len(yVal))\n",
    "print(confusion_matrix(yVal,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Dataset\n",
    "We will try to use Open Interest, Volume, Price from Nifty Futures and Price and Volume from Nifty from the last $LAG$ (variable, will adjust as fit) to make a prediction on whether nifty will increase or decrease as compared to $h$ days ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the data from the csv files\n",
    "nifty = pd.read_csv(\"NiftyPrice.csv\")\n",
    "nifty = nifty[::-1]\n",
    "nifty.reset_index(inplace=True, drop = True)\n",
    "\n",
    "futOI = pd.read_csv(\"NiftyFutOI.csv\")\n",
    "futOI = futOI[::-1]\n",
    "futOI.reset_index(inplace=True, drop = True)\n",
    "\n",
    "fut = pd.read_csv(\"NiftyFutPrice.csv\")\n",
    "fut = fut[::-1]\n",
    "fut.reset_index(inplace=True, drop = True)\n",
    "\n",
    "oil = pd.read_csv(\"CrudeOil.csv\")\n",
    "oil = oil[::-1]\n",
    "oil.reset_index(inplace=True, drop = True)\n",
    "\n",
    "# Drop any rows not common in both\n",
    "\n",
    "futDrop = np.setdiff1d(nifty['Date'].values,fut['Date'].values)\n",
    "niftyDrop = np.setdiff1d(fut['Date'].values,nifty['Date'].values)\n",
    "\n",
    "nifty = nifty[~nifty.Date.isin(futDrop)]\n",
    "fut = fut[~fut.Date.isin(niftyDrop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create % change daily bars of the different features\n",
    "# Nifty Base\n",
    "h = 2 # change from how far ago\n",
    "volNifty = nifty['PX_VOLUME'].pct_change(h).dropna()*100\n",
    "dataNifty = nifty['PX_LAST'].pct_change(h).dropna()*100\n",
    "\n",
    "# Nifty Futs\n",
    "volFut = fut['PX_VOLUME'].pct_change(h).dropna()*100\n",
    "oiFut = futOI['OPEN_INT'].pct_change(h).dropna()*100\n",
    "dataFut = fut['PX_LAST'].pct_change(h).dropna()*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 12\n",
    "lookahead = 1\n",
    "flat = 0\n",
    "# First N predicts N+1th. Creating the 1st N series\n",
    "closeNifty = timeseriesLagged(dataNifty,lag + lookahead-1)\n",
    "\n",
    "volNifty = timeseriesLagged(volNifty,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "volNifty = skp.scale(volNifty,axis=1)\n",
    "\n",
    "# First N predicts N+1th. Creating the 1st N series\n",
    "closeFut = timeseriesLagged(dataFut,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "closeFut = skp.scale(closeFut,axis=1)\n",
    "\n",
    "volFut = timeseriesLagged(volFut,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "volFut = skp.scale(volFut,axis=1)\n",
    "\n",
    "oiFut = timeseriesLagged(oiFut,lag + lookahead-1).drop(str(lag+1),axis=1)\n",
    "oiFut = skp.scale(oiFut,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binazrize the last column\n",
    "closeNifty.loc[closeNifty[str(lag+1)] > flat,str(lag+1)] = 1\n",
    "closeNifty.loc[closeNifty[str(lag+1)] <= flat,str(lag+1)] = 2\n",
    "closeNifty.loc[closeNifty[str(lag+1)] == flat,str(lag+1)] = 3\n",
    "closeNifty[str(lag+1)]=closeNifty[str(lag+1)].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the feature vector\n",
    "n = 5\n",
    "buySeriesLabs = closeNifty[str(lag+1)] # labels\n",
    "buySeriesFeats = closeNifty.drop(str(lag+1),axis=1) #features\n",
    "buySeriesFeats = buySeriesFeats.values\n",
    "buySeriesFeats = skp.scale(buySeriesFeats,axis=1)\n",
    "\n",
    "buySeries = np.zeros((len(buySeriesFeats),buySeriesFeats.shape[-1],n))\n",
    "buySeries[:,:,0] = buySeriesFeats\n",
    "buySeries[:,:,1] = volFut\n",
    "buySeries[:,:,2] = oiFut\n",
    "buySeries[:,:,3] = closeFut\n",
    "buySeries[:,:,4] = volNifty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train test val\n",
    "x,y = buySeries,buySeriesLabs\n",
    "x,y = shuffle(x,y)\n",
    "tot = len(x)\n",
    "y = y.values\n",
    "yOrig = np.copy(y)\n",
    "\n",
    "x = x.reshape(len(x),-1)\n",
    "trainPercent = 0.7 # majority of data used for training\n",
    "testPercent = 0.7 # \n",
    "valPercent = 1.00  #\n",
    "\n",
    "# Test Train Val Split\n",
    "\n",
    "xTrain = x[0:int(trainPercent*tot),:]\n",
    "yTrain = y[0:int(trainPercent*tot)]\n",
    "\n",
    "xTest = x[int(trainPercent*tot): int(testPercent*tot),:]\n",
    "yTest = y[int(trainPercent*tot): int(testPercent*tot)]\n",
    "\n",
    "xVal = x[int(testPercent*tot):,:]\n",
    "yVal = y[int(testPercent*tot):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes\n",
    "# nv = GaussianNB()\n",
    "# nv.fit(x,y)\n",
    "# preds=nv.predict(xVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg = SGDClassifier(loss='modified_huber',shuffle=True)\n",
    "# sg.fit(x,y)\n",
    "# preds = sg.predict(xVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Logistic Regression\n",
    "# lr = LogisticRegression()\n",
    "# lr.fit(x,y)\n",
    "# preds=lr.predict(xVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=2, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rbf SVM\n",
    "clf = svm.SVC(kernel='rbf', degree = 2, probability= True) # change kernels between linear, poly and rbf as fit.\n",
    "clf.fit(xTrain, yTrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predsProbs = clf.predict_log_proba(xVal)\n",
    "# np.argmax(predsProbs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    }
   ],
   "source": [
    "predsProbs = clf.predict_proba(xVal)\n",
    "preds = clf.predict(xVal)\n",
    "print(\"Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[306 105]\n",
      " [141 237]]\n",
      "Weight of Buy in data set: 0.520913\n"
     ]
    }
   ],
   "source": [
    "conf = confusion_matrix(yVal,preds)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "print(conf)\n",
    "print(\"Weight of Buy in data set: %f\" % (sum(yVal==1)/len(yVal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Percent: 0.688213\n",
      "Precision of Buy Model= 0.692982\n",
      "Recall of Buy Model = 0.626984\n"
     ]
    }
   ],
   "source": [
    "# Precision Recall of Sell\n",
    "print(\"Accuracy Percent: %f\"% (sum(preds==yVal)/len(yVal)))\n",
    "print(\"Precision of Buy Model= %f\" % (tp/(tp+fp)))\n",
    "print(\"Recall of Buy Model = %f\" % (tp/(tp+fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Percent: 0.688213\n",
      "Precision of Sell Model= 0.684564\n",
      "Recall of Sell Model = 0.744526\n",
      "Weight of Sell in data set: 0.479087\n"
     ]
    }
   ],
   "source": [
    "# Precision Recall of Sell\n",
    "print(\"Accuracy Percent: %f\"% (sum(preds==yVal)/len(yVal)))\n",
    "print(\"Precision of Sell Model= %f\" % (tn/(tn+fn)))\n",
    "print(\"Recall of Sell Model = %f\" % (tn/(tn+fp)))\n",
    "print(\"Weight of Sell in data set: %f\" % (sum(yVal==2)/len(yVal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
